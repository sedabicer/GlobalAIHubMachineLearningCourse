{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Homework1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJ3qJyniINEmDlfKGhOTww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sedabicer/GlobalAIHubMachineLearningCourse/blob/main/Homeworks/ML_Homework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "072G9ZhptI7Q"
      },
      "source": [
        "##1) How would you define Machine Learning?\n",
        "It is the science of getting machines to learn and act in a similar way to humans while also learning from real world interactions and sets of teaching data that we feed them.\n",
        "##2)What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these.\n",
        "                                 **1.Method**\n",
        "\n",
        "*Supervised Learning*: Input variables and output variables will be given.\n",
        "\n",
        "*Unsupervised Learning*: Only input data will be given.\n",
        "\n",
        "                                  **2.Goal**\n",
        "\n",
        "*Supervised Learning*: Supervised learning goal is to determine the function so well that when new input data set given, can predict the output.\n",
        "\n",
        "*Unsupervised Learning*: Unsupervised learning goal is to model the hidden patterns or underlying structure in the given input data in order to learn about the data.\n",
        "\n",
        "                                  **3.Uses**\n",
        "\n",
        "*Supervised Learning*: Supervised learning is often used for export systems in image recognition, speech recognition, forecasting, financial analysis and training neural networks and decision trees etc.\n",
        "\n",
        "*Unsupervised Learning*: Unsupervised learning algorithms are used to pre-process the data, during exploratory analysis or to pre-train supervised learning algorithms.\n",
        "\n",
        "                                 **4.Examples**\n",
        "                              \n",
        "*Supervised Learning*: Classification, Regression, Linear regression, Support vector machine.\n",
        "\n",
        "*Unsupervised Learning*: Clustering, Association, k-means, Association.\n",
        "##3) What are the test and validation set, and why would you want to use them?\n",
        "\n",
        "->Validation set: a set of examples used to tune the parameters of a classifier In the MLP case, we would use the validation set to find the “optimal” number of hidden units or determine a stopping point for the back-propagation algorithm\n",
        "\n",
        "->Test set: a set of examples used only to assess the performance of a fully-trained classifier In the MLP case, we would use the test to estimate the error rate after we have chosen the final model (MLP size and actual weights) After assessing the final model on the test set, YOU MUST NOT tune the model any further!\n",
        "\n",
        "##4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n",
        "###*Step 1: Import Libraries:*\n",
        "\n",
        "First step is usually importing the libraries that will be needed in the program. A library is essentially a collection of modules that can be called and used. A lot of the things in the programming world do not need to be written explicitly ever time they are required. There are functions for them, which can simply be invoked. This is a list for most popular Python libraries for Data Science. Here’s a snippet of me importing the pandas library and assigning a shortcut “pd”.\n",
        "\n",
        "###*Step 2: Import the Dataset:*\n",
        "\n",
        "A lot of datasets come in CSV formats. We will need to locate the directory of the CSV file at first (it’s more efficient to keep the dataset in the same directory as your program) and read it using a method called read_csv which can be found in the library called pandas.\n",
        "After inspecting our dataset carefully, we are going to create a matrix of features in our dataset (X) and create a dependent vector (Y) with their respective observations. To read the columns, we will use iloc of pandas (used to fix the indexes for selection) which takes two parameters — [row selection, column selection].\n",
        "\n",
        "###*Step 3: Taking care of Missing Data in Dataset:*\n",
        "\n",
        "Sometimes you may find some data are missing in the dataset. We need to be equipped to handle the problem when we come across them. Obviously you could remove the entire line of data but what if you are unknowingly removing crucial information? Of course we would not want to do that. One of the most common idea to handle the problem is to take a mean of all the values of the same column and have it to replace the missing data.\n",
        "The library that we are going to use for the task is called Scikit Learn preprocessing. It contains a class called Imputer which will help us take care of the missing data.\n",
        "A lot of the times the next step, as you will also see later on in the article, is to create an object of the same class to call the functions that are in that class. We will call our object imputer. The Imputer class can take a few parameters —\n",
        "i. missing_values — We can either give it an integer or “NaN” for it to find the missing values.\n",
        "ii. strategy — we will find the average so we will set it to mean. We can also set it to median or most_frequent (for mode) as necessary.\n",
        "iii. axis — we can either assign it 0 or 1, 0 to impute along columns and 1 to impute along rows.\n",
        "\n",
        "###*Step 4: Encoding categorical data:*\n",
        "\n",
        "Sometimes our data is in qualitative form, that is we have texts as our data. We can find categories in text form. Now it gets complicated for machines to understand texts and process them, rather than numbers, since the models are based on mathematical equations and calculations. Therefore, we have to encode the categorical data.\n",
        "\n",
        "###*Step 5: Splitting the Dataset into Training set and Test Set:*\n",
        "\n",
        "Now we need to split our dataset into two sets — a Training set and a Test set. We will train our machine learning models on our training set, i.e our machine learning models will try to understand any correlations in our training set and then we will test the models on our test set to check how accurately it can predict. A general rule of the thumb is to allocate 80% of the dataset to training set and the remaining 20% to test set. For this task, we will import test_train_split from model_selection library of scikit.\n",
        "\n",
        "###*Step 6: Feature Scaling:*\n",
        "\n",
        "The final step of data preprocessing is to apply the very important feature scaling.\n",
        "\n",
        "But what is it?\n",
        "\n",
        "It is a method used to standardize the range of independent variables or features of data.\n",
        "\n",
        "But why is it necessary?\n",
        "\n",
        "A lot of machine learning models are based on Euclidean distance. If, for example, the values in one column (x) is much higher than the value in another column (y), (x2-x1) squared will give a far greater value than (y2-y1) squared. So clearly, one square difference dominates over the other square difference. In the machine learning equations, the square difference with the lower value in comparison to the far greater value will almost be treated as if it does not exist. We do not want that to happen. That is why it is necessary to transform all our variables into the same scale. There are several ways of scaling the data. One way is called Standardization which may be used. For every observation of the selected column, our program will apply the formula of standardization and fit it to a scale.\n",
        "\n",
        "##5) How you can explore countionus and discrete variables?\n",
        "->Discrete variables represent counts (e.g. the number of objects in a collection).\n",
        "->Continuous variables represent measurable amounts (e.g. water volume or weight).\n",
        "\n",
        "##6) Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)\n",
        "\n",
        "The graph have continuous variable, also it would be bimodal distribution with normalization.I think that it would be transform to the normal distribution from bimodal by using quantile transform. Then I can detect the outliers. After detecting and eliminating the outliers I can normilize the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}